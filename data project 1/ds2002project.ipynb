{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "### Getting first data source\n",
        "# Source 2: a local file\n",
        "# Top 100 songs with the most streams on Spotify\n",
        "# Source --> https://www.kaggle.com/code/ludovicocuoghi/spotify-top-100-streamed-songs-analysis/input\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "print(\"Getting first data source...\")\n",
        "print(\"...reading...\")\n",
        "try:\n",
        "    top_100_file_path = \"/content/Top 100 most Streamed - Sheet1.csv\"\n",
        "    top_100_df = pd.read_csv(top_100_file_path)\n",
        "    print(\"...first data source received...\")\n",
        "    print(f\"...read in {top_100_df.shape[0]} rows (songs) across {top_100_df.shape[1]} columns\")\n",
        "except Exception as e:\n",
        "    print(\"...failed to read given static source- ensure the file is in the content ('/content/<file>') directory in Google Colab\")\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMfM0wFxk02t",
        "outputId": "2ab00de2-6052-480a-daca-491d6d7f9e94"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting first data source...\n",
            "...reading...\n",
            "...first data source received...\n",
            "...read in 100 rows (songs) across 14 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Getting second data source\n",
        "# Source 1: API data\n",
        "# Using musicbrainz to match each artist/group in Source 2 with their country\n",
        "# Using multiple API calls to make a new dataset\n",
        "\n",
        "import requests\n",
        "import time\n",
        "\n",
        "\n",
        "try:\n",
        "    artist_locations_df = pd.DataFrame(columns=[\"artist\", \"country\"])\n",
        "    index = 0\n",
        "\n",
        "    url = \"https://musicbrainz.org/ws/2/artist\"\n",
        "    artist_list = top_100_df[\"artist\"]\n",
        "    seen = set() # tracking queried artists to avoid API calls for speed\n",
        "\n",
        "    print(\"Getting second data source...\")\n",
        "    print(\"...querying endpoints (takes around a minute because of intentional query delays)...\")\n",
        "    for artist in artist_list:\n",
        "        if artist not in seen:\n",
        "            # standard API call\n",
        "            params = {\n",
        "                \"query\": f'artist:\"{artist}\"',\n",
        "                \"fmt\": \"json\"\n",
        "            }\n",
        "            headers = {\n",
        "                \"User-Agent\": \"ArtistLocations/2.0 ( fakeemail@example.com )\"  # had to work around musicbrainz for query throttling\n",
        "            }\n",
        "            try:\n",
        "                time.sleep(0.5)\n",
        "                response = requests.get(url, headers=headers, params=params)\n",
        "                artist_data = response.json()[\"artists\"][0]\n",
        "\n",
        "                # storing response in new df (building the data set)\n",
        "                artist_locations_df.loc[index] = [artist, artist_data[\"country\"]]\n",
        "                index += 1\n",
        "            except Exception as e:\n",
        "                print(\"\\t...musicbrainz could not gather info on \" + artist + \"...\")\n",
        "            seen.add(artist)\n",
        "    print(\"...done gathering artist info, second dataset built...\")\n",
        "    print(f\"...read in {artist_locations_df.shape[0]} rows (artists) across {artist_locations_df.shape[1]} columns\")\n",
        "except Exception as e:\n",
        "    print('...failed to query MusicBrainz API, make sure previous code cell has been run')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os85lirYlIE-",
        "outputId": "09cf03a2-ee2d-4259-ed6a-bcc44f1d62fc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting second data source...\n",
            "...querying endpoints (takes around a minute because of intentional query delays)...\n",
            "\t...musicbrainz could not gather info on Harry Styles...\n",
            "\t...musicbrainz could not gather info on Calvin Harris...\n",
            "\t...musicbrainz could not gather info on Mark Mendy...\n",
            "...done gathering artist info, second dataset built...\n",
            "...read in 61 rows (artists) across 2 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Merging + Analysis\n",
        "\n",
        "print(\"Starting analysis of datasets...\")\n",
        "# merging and cleaning df\n",
        "print(\"...merging and cleaning datasets...\")\n",
        "try:\n",
        "    merged_df = top_100_df.merge(artist_locations_df, on=\"artist\", how=\"inner\")\n",
        "    merged_df.drop(columns=[\"liveness\", \"year\", \"popularity\", \"acousticness\", \"speechiness\", \"loudness.dB\"], inplace=True)\n",
        "    print(f\"...merged datasets into {merged_df.shape[0]} rows (songs + artist info) across {merged_df.shape[1]} columns\\n\\n\")\n",
        "\n",
        "    current_area_bpm_df = merged_df.groupby(\"country\")[\"top genre\"]\n",
        "    print(current_area_bpm_df.describe())\n",
        "    print(\"--------------\\n\")\n",
        "    print(\"...grouping by country area and bpm...\")\n",
        "    current_area_bpm_df = merged_df.groupby(\"country\")[\"beats.per.minute\"]\n",
        "    print(current_area_bpm_df.describe())\n",
        "    print(\"--------------\\n\")\n",
        "    print(\"...grouping by country area and energy...\")\n",
        "    current_area_bpm_df = merged_df.groupby(\"country\")[\"energy\"]\n",
        "    print(current_area_bpm_df.describe())\n",
        "    print(\"--------------\\n\")\n",
        "    print(\"...grouping by country area and danceability...\")\n",
        "    current_area_bpm_df = merged_df.groupby(\"country\")[\"danceability\"]\n",
        "    print(current_area_bpm_df.describe())\n",
        "    print(\"--------------\\n\")\n",
        "    print(\"...grouping by country area and valance...\")\n",
        "    current_area_bpm_df = merged_df.groupby(\"country\")[\"valance\"]\n",
        "    print(current_area_bpm_df.describe())\n",
        "    print(\"--------------\\n\")\n",
        "    print(\"...grouping by country and length...\")\n",
        "    current_area_bpm_df = merged_df.groupby(\"country\")[\"length\"]\n",
        "    print(current_area_bpm_df.describe())\n",
        "    print(\"--------------\\n\")\n",
        "\n",
        "    print(\"...analysis complete, formatted tables printed above and more specific conclusions found in the code comments\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"...analysis failed, make sure all previous code blocks have been run\")\n",
        "    print(e)\n",
        "\n",
        "\n",
        "\n",
        "### Analysis Notes\n",
        "# Note --> not a huge dataset, could effect analysis based on location if only a few songs per location (but that's not the focus of this project)\n",
        "# Genre\n",
        "    # Pop (or some variation of it) is by far the most popular amongst all countries\n",
        "# BPM - beats-per-minute; the tempo of the song\n",
        "    # US, GB, and Canada had high variability (std), as compared to Australia, Peurto Rico and Jamacia\n",
        "    # Peurto Rico very high mean, Norway very low mean\n",
        "# Energy - how energetic a song is (1-100)\n",
        "    # Jamacia and Puertro Rico had low std's (outlier)\n",
        "    # Puerto Rico, Sweden, and Jamacia had a very high mean\n",
        "    # US, UK, Canada all had means on the lower end\n",
        "# Danceability - how easy a song is to dance to (1-100)\n",
        "    # Puertro Rico had low std (outlier)\n",
        "    # high mean with Guyana, Jamacia, and Demark\n",
        "    # Lower mean for Sweden and Norway\n",
        "# Valance - the mood of the song, high = positive(1-100)\n",
        "    # Puerto Rico very low std (outlier)\n",
        "    # Puerto Rico also very high mean valance along side Guyana\n",
        "    # Norway and France low mean\n",
        "# Length - duration (seconds)\n",
        "    # Low variation in Australia, Jamacia, and Peutro Rico\n",
        "    # Guyana and Jamacia shorter mean\n",
        "    # Great Britain and Sweden higher mean\n",
        "# Overall\n",
        "    # Pop (or some variation of it) is by far the most popular regardless of location\n",
        "    # Puerto Rico stands out with high BPM, energy, and valance.\n",
        "    # The US, Canada, and Great Britain have the most variation across all levels, suggesting more diverse types of songs (also maybe attributed to more top songs in the dataset).\n",
        "    # There does seem to be some correlation with where an Artist is from and the attributes of their songs across the top 100 songs on Spotify\n",
        "        # Somewhat hard to identify this trend across regions because only have country data, not country specifics (like weather, coordinates, etc.)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRaHYMzE1YlP",
        "outputId": "5e98253f-def4-4330-b8ba-fb512cff1759"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting analysis of datasets...\n",
            "...merging and cleaning datasets...\n",
            "...merged datasets into 97 rows (songs + artist info) across 9 columns\n",
            "\n",
            "\n",
            "        count unique                      top freq\n",
            "country                                           \n",
            "AU          4      4           australian pop    1\n",
            "CA         12      3             canadian pop    6\n",
            "DK          1      1               danish pop    1\n",
            "FR          1      1                dance pop    1\n",
            "GB         15      5                      pop    7\n",
            "GY          1      1              melodic rap    1\n",
            "IE          1      1  irish singer-songwriter    1\n",
            "JM          3      1                dance pop    3\n",
            "NO          1      1            electro house    1\n",
            "PR          2      1                    latin    2\n",
            "SE          1      1                dance pop    1\n",
            "US         55     20                dance pop   17\n",
            "--------------\n",
            "\n",
            "...grouping by country area and bpm...\n",
            "         count        mean        std    min    25%    50%     75%    max\n",
            "country                                                                  \n",
            "AU         4.0  102.500000  12.688578   90.0   96.0  100.0  106.50  120.0\n",
            "CA        12.0  119.500000  33.513905   77.0  100.0  110.5  131.25  186.0\n",
            "DK         1.0  120.000000        NaN  120.0  120.0  120.0  120.00  120.0\n",
            "FR         1.0  100.000000        NaN  100.0  100.0  100.0  100.00  100.0\n",
            "GB        15.0  101.800000  25.490054   71.0   84.5   96.0  112.50  174.0\n",
            "GY         1.0  122.000000        NaN  122.0  122.0  122.0  122.00  122.0\n",
            "IE         1.0  129.000000        NaN  129.0  129.0  129.0  129.00  129.0\n",
            "JM         3.0  103.000000  13.228757   93.0   95.5   98.0  108.00  118.0\n",
            "NO         1.0   90.000000        NaN   90.0   90.0   90.0   90.00   90.0\n",
            "PR         2.0  178.000000   0.000000  178.0  178.0  178.0  178.00  178.0\n",
            "SE         1.0  124.000000        NaN  124.0  124.0  124.0  124.00  124.0\n",
            "US        55.0  120.563636  26.336683   75.0   97.0  120.0  140.00  171.0\n",
            "--------------\n",
            "\n",
            "...grouping by country area and energy...\n",
            "         count       mean        std   min    25%   50%    75%   max\n",
            "country                                                             \n",
            "AU         4.0  71.750000  10.688779  59.0  67.25  71.5  76.00  85.0\n",
            "CA        12.0  62.750000  13.830960  38.0  55.50  60.0  75.25  82.0\n",
            "DK         1.0  47.000000        NaN  47.0  47.00  47.0  47.00  47.0\n",
            "FR         1.0  72.000000        NaN  72.0  72.00  72.0  72.00  72.0\n",
            "GB        15.0  55.400000  16.330515  37.0  41.50  54.0  66.50  90.0\n",
            "GY         1.0  72.000000        NaN  72.0  72.00  72.0  72.00  72.0\n",
            "IE         1.0  66.000000        NaN  66.0  66.00  66.0  66.00  66.0\n",
            "JM         3.0  76.666667   6.658328  69.0  74.50  80.0  80.50  81.0\n",
            "NO         1.0  65.000000        NaN  65.0  65.00  65.0  65.00  65.0\n",
            "PR         2.0  80.000000   0.000000  80.0  80.00  80.0  80.00  80.0\n",
            "SE         1.0  78.000000        NaN  78.0  78.00  78.0  78.00  78.0\n",
            "US        55.0  61.018182  17.316756  11.0  51.50  62.0  73.50  92.0\n",
            "--------------\n",
            "\n",
            "...grouping by country area and danceability...\n",
            "         count       mean        std   min    25%   50%    75%   max\n",
            "country                                                             \n",
            "AU         4.0  63.250000  14.080128  48.0  57.00  61.5  67.75  82.0\n",
            "CA        12.0  67.416667  11.988315  44.0  60.50  69.5  75.25  85.0\n",
            "DK         1.0  77.000000        NaN  77.0  77.00  77.0  77.00  77.0\n",
            "FR         1.0  65.000000        NaN  65.0  65.00  65.0  65.00  65.0\n",
            "GB        15.0  61.533333  18.216424  36.0  43.50  61.0  78.50  86.0\n",
            "GY         1.0  77.000000        NaN  77.0  77.00  77.0  77.00  77.0\n",
            "IE         1.0  57.000000        NaN  57.0  57.00  57.0  57.00  57.0\n",
            "JM         3.0  70.333333   8.621678  61.0  66.50  72.0  75.00  78.0\n",
            "NO         1.0  59.000000        NaN  59.0  59.00  59.0  59.00  59.0\n",
            "PR         2.0  65.500000   0.707107  65.0  65.25  65.5  65.75  66.0\n",
            "SE         1.0  53.000000        NaN  53.0  53.00  53.0  53.00  53.0\n",
            "US        55.0  68.745455  13.462180  35.0  62.50  72.0  76.50  91.0\n",
            "--------------\n",
            "\n",
            "...grouping by country area and valance...\n",
            "         count       mean        std   min    25%   50%   75%   max\n",
            "country                                                            \n",
            "AU         4.0  47.500000  24.020824  15.0  42.00  51.0  56.5  73.0\n",
            "CA        12.0  52.500000  20.496119  14.0  39.75  50.5  75.0  79.0\n",
            "DK         1.0  34.000000        NaN  34.0  34.00  34.0  34.0  34.0\n",
            "FR         1.0  16.000000        NaN  16.0  16.00  16.0  16.0  16.0\n",
            "GB        15.0  49.733333  26.724699  17.0  22.00  48.0  64.5  93.0\n",
            "GY         1.0  90.000000        NaN  90.0  90.00  90.0  90.0  90.0\n",
            "IE         1.0  44.000000        NaN  44.0  44.00  44.0  44.0  44.0\n",
            "JM         3.0  45.666667  16.921387  27.0  38.50  50.0  55.0  60.0\n",
            "NO         1.0  17.000000        NaN  17.0  17.00  17.0  17.0  17.0\n",
            "PR         2.0  85.000000   1.414214  84.0  84.50  85.0  85.5  86.0\n",
            "SE         1.0  64.000000        NaN  64.0  64.00  64.0  64.0  64.0\n",
            "US        55.0  48.545455  20.232737   6.0  33.50  45.0  65.0  91.0\n",
            "--------------\n",
            "\n",
            "...grouping by country and length...\n",
            "         count        mean        std    min     25%    50%     75%    max\n",
            "country                                                                   \n",
            "AU         4.0  207.000000   4.242641  203.0  203.75  206.5  209.75  212.0\n",
            "CA        12.0  207.166667  19.971950  174.0  197.00  203.5  218.00  242.0\n",
            "DK         1.0  237.000000        NaN  237.0  237.00  237.0  237.00  237.0\n",
            "FR         1.0  206.000000        NaN  206.0  206.00  206.0  206.00  206.0\n",
            "GB        15.0  236.866667  47.654810  173.0  205.00  234.0  261.00  354.0\n",
            "GY         1.0  177.000000        NaN  177.0  177.00  177.0  177.00  177.0\n",
            "IE         1.0  242.000000        NaN  242.0  242.00  242.0  242.00  242.0\n",
            "JM         3.0  181.000000   4.000000  177.0  179.00  181.0  183.00  185.0\n",
            "NO         1.0  213.000000        NaN  213.0  213.00  213.0  213.00  213.0\n",
            "PR         2.0  229.500000   0.707107  229.0  229.25  229.5  229.75  230.0\n",
            "SE         1.0  247.000000        NaN  247.0  247.00  247.0  247.00  247.0\n",
            "US        55.0  212.745455  36.506325  119.0  191.50  214.0  234.50  321.0\n",
            "--------------\n",
            "\n",
            "...analysis complete, formatted tables printed above and more specific conclusions found in the code comments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate SQL Database\n",
        "import sqlite3\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Inserting data into a SQL database...\")\n",
        "try:\n",
        "    # Creating empty file with sqlite database\n",
        "    Path(\"data_project_1.db\").touch()\n",
        "    print(\"...empty database created...\")\n",
        "\n",
        "    # Connect to database\n",
        "    conn = sqlite3.connect(\"data_project_1.db\")\n",
        "    merged_df.to_sql(\"TopSongsWithArea\", conn, if_exists=\"replace\", index=False)\n",
        "    print(\"...database filled...\")\n",
        "    conn.close()\n",
        "    print(\"...finished, connection closed\")\n",
        "except Exception as e:\n",
        "    print(\"...something went wrong when generating the SQL database, make sure all previous code blocks have been run\")\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "XrGxXargh27m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d23a6c5a-635f-458a-a888-52b308280b02"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inserting data into a SQL database...\n",
            "...empty database created...\n",
            "...database filled...\n",
            "...finished, connection closed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Return type to the user\n",
        "\n",
        "print(\"How would you like to return merged datasets? 'C' for a .csv file, 'J' for a .json file, or 'S' for SQL\")\n",
        "try:\n",
        "    return_type = str(input()).lower()\n",
        "    if return_type == \"c\":\n",
        "        print(\"...csv selected, generating file...\")\n",
        "        merged_df.to_csv(\"TopSongsWithArea.csv\", index=False)\n",
        "        print(\"...csv generated as TopSongsWithArea.csv in the /content/ directory\")\n",
        "    elif return_type == \"j\":\n",
        "        print(\"...json selected, generating file...\")\n",
        "        merged_df.to_json(\"TopSongsWithArea.json\", orient=\"records\")\n",
        "        print(\"...json generated as TopSongsWithArea.json in the /content/ directory\")\n",
        "    elif return_type == \"s\":\n",
        "        print(\"...SQL selected, generating file...\")\n",
        "        print(\"...connecting to the database...\")\n",
        "        try:\n",
        "            conn = sqlite3.connect(\"data_project_1.db\")\n",
        "            print(\"...connected, reading data...\")\n",
        "            with open(\"TopSongsWithArea.sql\", \"w\") as file:\n",
        "                for line in conn.iterdump():\n",
        "                    file.write(f\"{line}\\n\")\n",
        "            print(\"...SQL generated as TopSongsWithArea.sql in the /content/ directory\")\n",
        "        except Exception as e:\n",
        "            print(\"...error occured while connecting to the database or writing the .sql file. Make sure all previous code blocks have been run\")\n",
        "    else:\n",
        "        print(\"...please enter a valid input\")\n",
        "except Exception as e:\n",
        "    print(\"...something went wrong with your return input, please ensure it is a valid matching string and that all previous code blocks have been run\")\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_IiC7tlpSNl",
        "outputId": "b193c467-365a-4a52-d977-538363b74467"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How would you like to return merged datasets? 'C' for a .csv file, 'J' for a .json file, or 'S' for SQL\n",
            "s\n",
            "...SQL selected, generating file...\n",
            "...connecting to the database...\n",
            "...connected, reading data...\n",
            "...SQL generated as TopSongsWithArea.sql in the /content/ directory\n"
          ]
        }
      ]
    }
  ]
}